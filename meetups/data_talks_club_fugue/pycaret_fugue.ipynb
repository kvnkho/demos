{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Fugue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/fugue_architecture.png\" align=\"center\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret\n",
    "\n",
    "[PyCaret](https://github.com/pycaret/pycaret) is a low code machine learning framework that automates a lot of parts of the machine learning pipeline. With just a few lines of code, several models can be trained on a dataset. In this post, we explore how to scale this capability by running several PyCaret training jobs in a distributed manner on Spark or Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "df = get_data('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *\n",
    "clf = setup(data = df, target = 'Survived', session_id=123, silent = True, verbose=False, html=False)\n",
    "models = compare_models(fold = 3, sort = \"Accuracy\", turbo = True, verbose=False)\n",
    "results = pull().reset_index(drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Logic in Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    clf = setup(data = df, \n",
    "                target = 'Survived', \n",
    "                session_id=123, \n",
    "                silent = True, \n",
    "                verbose=False, \n",
    "                html=False)\n",
    "    models = compare_models(fold = 3,  \n",
    "                            sort = \"Accuracy\", \n",
    "                            turbo = True, \n",
    "                            verbose=False)\n",
    "    results = pull().reset_index(drop=True)\n",
    "\n",
    "    return pd.DataFrame(dict(model=results[\"Model\"], \n",
    "                             auc=results[\"AUC\"], \n",
    "                             recall=results[\"Recall\"],\n",
    "                             precision=results[\"Prec.\"],\n",
    "                             time=results[\"TT (Sec)\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fugue Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fugue import transform\n",
    "\n",
    "schema = \"\"\"model:str, auc:float, recall:float, precision:float, time:float\"\"\"\n",
    "\n",
    "res = transform(df, wrapper, schema=schema)\n",
    "res[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Male and Female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"sex:str, model:str, auc:float, recall:float, precision:float, time:float\"\"\"\n",
    "\n",
    "res = transform(df, wrapper, schema=schema, partition={\"by\":\"Sex\"})\n",
    "res.sort_values(\"auc\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = transform(df.replace({np.nan: None}), \n",
    "                wrapper, \n",
    "                schema=schema, \n",
    "                partition={\"by\":\"Sex\"}, \n",
    "                save_path=\"/tmp/results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FugueSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "LOAD \"/tmp/results.parquet\"\n",
    "PRINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "df = LOAD \"/tmp/results.parquet\"\n",
    "\n",
    "SELECT sex, AVG(time) AS time\n",
    "  FROM df\n",
    " GROUP BY sex\n",
    " PRINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting to Local DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql spark\n",
    "df = LOAD \"/tmp/results.parquet\"\n",
    "\n",
    "TAKE 5 ROWS FROM df PREPARTITION BY sex PRESORT auc DESC\n",
    "YIELD LOCAL DATAFRAME AS result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.native"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plotter(df:pd.DataFrame) -> None:\n",
    "\n",
    "    fig = plt.figure(figsize=(12,10))\n",
    "    ax = sns.scatterplot(x=df[\"precision\"],y=df[\"recall\"],hue=df[\"sex\"])\n",
    "    # The magic starts here:\n",
    "    for line in range(0,df.shape[0]):\n",
    "         ax.text(df[\"precision\"].iloc[line]+0.01, df[\"recall\"].iloc[line], \n",
    "                 df[\"model\"].iloc[line], horizontalalignment='left', \n",
    "                 size='medium', color='black', weight='semibold')\n",
    "\n",
    "    plt.title('Precision and Recall')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%fsql\n",
    "SELECT * \n",
    "  FROM result\n",
    " WHERE sex = 'male'\n",
    "\n",
    "OUTPUT USING plotter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fcd6e71927f6b3e5f4fa4280b4e8e6a66aa8d4365bb61cf7ef4017620fc09b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
